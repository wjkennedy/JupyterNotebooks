{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 2022 Jira Jupyter Gephi Disco"},{"metadata":{},"cell_type":"markdown","source":"Based on the examples in https://towardsdatascience.com/communication-story-from-an-issue-tracking-software-efbbf29736ff\n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import dependencies\nfrom jira import JIRA, JIRAError\nfrom collections import Counter, defaultdict\nfrom datetime import datetime\nfrom time import sleep\n\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\n\n\n# Create instance for interacting with Jira\njira = JIRA(options={'server': url}, basic_auth=(username, password))\n\n\n# Read data from Jira with changelog\njira_search = jira.search_issues(jql, startAt=block_num*block_size, maxResults=block_size, \n\t\tfields=\"issuetype, created, resolutiondate, reporter, assignee, status\", \n\t\texpand='changelog')\n\n\n# Define parameters for writing data\nindex_beg = 0\nheader = True\nmode = 'w'\n\n# Iteratively read data\nwhile bool(jira_search):\n# Container for Jira's data\n\tdata_jira = []\n\n\tfor issue in jira_search:\n# Get issue key\n\tissue_key = issue.key\n\n# Get request type\nrequest_type = str(issue.fields.issuetype)\n\n# Get datetime creation\n\tdatetime_creation = issue.fields.created\n\tif datetime_creation is not None:\n# Interested in only seconds precision, so slice unnecessary part\n\tdatetime_creation = datetime.strptime(datetime_creation[:19], \"%Y-%m-%dT%H:%M:%S\")\n\n# Get datetime resolution\n\tdatetime_resolution = issue.fields.resolutiondate\n\tif datetime_resolution is not None:\n# Interested in only seconds precision, so slice unnecessary part\n\tdatetime_resolution = datetime.strptime(datetime_resolution[:19], \"%Y-%m-%dT%H:%M:%S\")\n\n# Get reporter’s login and name\n\treporter_login = None\n\treporter_name = None\n\treporter = issue.raw['fields'].get('reporter', None)\n\tif reporter is not None:\n\treporter_login = reporter.get('key', None)\n\treporter_name = reporter.get('displayName', None)\n\n# Get assignee’s login and name\n\tassignee_login = None\n\tassignee_name = None\n\tassignee = issue.raw['fields'].get('assignee', None)\n\tif assignee is not None:\n\tassignee_login = assignee.get('key', None)\n\tassignee_name = assignee.get('displayName', None)\n\n# Get status\n\tstatus = None\n\tst = issue.fields.status\n\tif st is not None:\n\tstatus = st.name\n\n# Read data from Jira with changelog\n\tjira_search = jira.search_issues(jql, startAt=block_num*block_size, maxResults=block_size, \n\t\t\tfields=\"issuetype, created, resolutiondate, reporter, assignee, status\", \n\t\t\texpand='changelog')\n\n# Get information from changelog\n\thistory_assignee = []\n\thistories = issue.raw['changelog'].get('histories', None)\n\tif histories is not None:\n\tfor history in histories:\n\tfor item in history['items']:\n\tif item['field'] == 'assignee':\n# Get history author, previous assignee, new assignee\n\thistory_author = history.get('author', None)\n\tif history_author is not None:\n\thistory_author = history_author['key']\n\thistory_assignee.append([history_author, item['from'], item['to'], datetime.strptime(history['created'][:19], \"%Y-%m-%dT%H:%M:%S\")])\n\n# Add data to data_jira\ndata_jira.append((issue_key, request_type, datetime_creation, datetime_resolution, reporter_login, reporter_name, assignee_login, assignee_name, status))\n\n# Write data read from Jira\nindex_end = index_beg + len(data_jira)\n\tdata_jira = pd.DataFrame(data_jira, index=range(index_beg, index_end), \n\t\t\tcolumns=['Issue key', 'Request type', 'Datetime creation', 'Datetime resolution', 'Reporter login', 'Reporter name', 'Assignee login', 'Assignee name', 'Status'])\n\tdata_jira.to_csv(path_or_buf='data_jira.csv', sep=';', header=header, index=True, index_label='N', mode=mode)\n\n# Update for the next iteration\n\tblock_num = block_num + 1\n\tindex_beg = index_end\n\theader = False\n\tmode = 'a'\n\n# Print how many issues were read\n\tif block_num % 50 == 0:\nprint(block_num * block_size)\n\n# Pause before next reading – it’s optional, just to be sure we will not overload Jira’s server\nsleep(1)\n\n# New issues search\n\tjira_search = jira.search_issues(jql, startAt=block_num*block_size, maxResults=block_size, \n\t\t\tfields=\"issuetype, created, resolutiondate, reporter, assignee, status\")\n\njira.close()\n\texcept (JIRAError, AttributeError):\n\t\tjira.close()\n\t\tprint('Error')\n\n\n\n# Read data from created before csv file\nreporter_assignee = defaultdict(int)\ncreation = {}\nchunksize = 1000\nfor chunk in pd.read_csv('data_jira.csv', sep=';', header=0, usecols=['Reporter login', 'Assignee login', 'Datetime creation'], engine='python', chunksize=chunksize, encoding='utf_8_sig'):\n    for i in chunk.index:\n        reporter = chunk.loc[i, 'Reporter login']\n        assignee = chunk.loc[i, 'Assignee login']\n        data_beg = datetime.strptime(chunk.loc[i, 'Datetime creation'], \"%Y-%m-%d %H:%M:%S\").date()\n        if reporter is not np.nan and assignee is not np.nan: \n            reporter_assignee[(reporter, assignee)] += 1\n            if creation.get(reporter, None) is None:\n                creation[reporter] = {'creation': data_beg}\n            else:\n                if data_beg < creation[reporter]['creation']:\n                    creation[reporter] = {'creation': data_beg}\n            if creation.get(assignee, None) is None:\n                creation[assignee] = {'creation': data_beg}\n            else:\n                if data_beg < creation[assignee]['creation']:\n                    creation[assignee] = {'creation': data_beg}\n\n# Create graph\ngraph_units = nx.DiGraph()\ngraph_units.add_weighted_edges_from([(x[0], x[1], y) for x, y in reporter_assignee.items()])\n\n# Set for every node date creation\nfor item in creation:\n    creation[item]['creation'] = creation[item]['creation'].strftime(\"%Y-%m-%d\")\nnx.set_node_attributes(graph_units, creation)\n\n# Set a color for nodes according business units (business units read from special file, so this code may be dropped)\nunit_colors = {}\nfor item in Counter(active_dir['NAME_UNIT']).keys():\n    unit_colors[item] = '#{:02x}{:02x}{:02x}'.format(random.randint(0,240), random.randint(0,255), random.randint(0,255))\ncolors = {}\nfor item in graph_units.nodes:\n    employee = active_dir[active_dir['LOGIN'] == item]['IBLOCK_SECTION_NAME_2']\n    if not employee.empty:\n        if employee.values[0] is not np.nan:\n            colors[item] = {'color': unit_colors[employee.values[0]]}\n        else:\n            colors[item] = {'color': '#ff0000'} # #ff0000 - red color\n    else:\n        colors[item] = {'color': '#ff0000'}\nnx.set_node_attributes(graph_units, colors)\n\n# Save graph\nnx.write_graphml(graph_units, 'graph_employee.graphml')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now go get Gephi and/or Disco to load the two data files:\n\n- data_jira.csv into Disco\n- graph_employee.graphml into Gephi"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}